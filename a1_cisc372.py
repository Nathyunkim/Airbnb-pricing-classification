# -*- coding: utf-8 -*-
"""A1_CISC372.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f86KRT7HOo6SpdZofnP93m6PEPZcbZet

Load the data
"""

# download data (-q is the quiet mode)
! wget -q https://www.dropbox.com/s/lhb1awpi769bfdr/test.csv?dl=1 -O test.csv
! wget -q https://www.dropbox.com/s/gudb5eunj700s7j/train.csv?dl=1 -O train.csv

import pandas as pd

Xy_train = pd.read_csv('train.csv', engine='python')
# X_train is the training data except for the price_rating feature that we want to predict 
X_train = Xy_train.drop(columns=['price_rating'])
# Y_train is the price_rating feature from the training data 
y_train = Xy_train[['price_rating']]

# print size of training set
print('traning', len(X_train))
# show distribution of price_rating in training data
Xy_train.price_rating.hist()

# print size of testing set
X_test = pd.read_csv('test.csv', engine='python')
testing_ids = X_test.Id
print('testing', len(X_test))

# model training and tuning
import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.datasets import fetch_openml
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import f1_score
from xgboost.sklearn import XGBClassifier

# makes the random numbers the same 
np.random.seed(0)

# creates array of numeric features 
numeric_features = ['bedrooms', 'accommodates', 'beds', 'availability_30',
                    'number_of_reviews', 'minimum_nights', 
                    'review_scores_rating', 'review_scores_location', 
                    'reviews_per_month']

# encapsulation of transformers 
# - replace missing values with the median 
# - data standardization (gaussian with zero mean and unit variance)
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

# creates array of categorical features 
categorical_features = [
  'property_type', 'is_business_travel_ready', 'room_type', 'neighbourhood', 
  'host_is_superhost']

# encapsulation of transformers 
# - replace missing values with 'missing'
# - apply one-hot encoding to the categorical features, ignore unknown feature
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

# applies numeric_transformer to numeric columns 
# applies categorical_tranformer to categorical columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# Our estimator 
# - apply preprocessor we created 
# - use XGBoost as our regressor 
# - use softmax as our objective function 
regr = Pipeline(steps=[('preprocessor', preprocessor),
                      ('regressor', XGBClassifier(
                          objective='multi:softmax', seed=1))])

# recompose training and testing datasets with transformed data
X_train = X_train[[*numeric_features, *categorical_features]]
X_test = X_test[[*numeric_features, *categorical_features]]

# `__` denotes attribute 
# (e.g. regressor__n_estimators means the `n_estimators` param for `regressor`
#  which is our xgb)
# hyper-parameters to be explored and allowed values 
param_grid = {
    'preprocessor__num__imputer__strategy': ['mean'],
    'regressor__n_estimators': [50, 100],
    'regressor__max_depth':[10, 20],
    'regressor__min_child_weight':[4,5,6],
    'regressor__learning_rate': [0.1, 0.01]
}

# hyper-parameter tuning 
# exaustively explores values from param_grid
# use 5-fold cross validation
grid_search = GridSearchCV(
    regr, param_grid, cv=5, verbose=3, n_jobs=2, 
    scoring='f1_micro')

# fit the model to the training data
grid_search.fit(X_train, y_train)

# print mean cross-validated score of the best_estimator
print('best score {}'.format(grid_search.best_score_))

# try logistic regression 
from sklearn.linear_model import LogisticRegression

# estimator using logistic regression 
logregr = Pipeline(steps=[('preprocessor', preprocessor),
                      ('regressor', LogisticRegression())])

# hyper-parameters to be explored and allowed values
param_grid = {
    'preprocessor__num__imputer__strategy': ['mean'],
    'regressor__max_iter': [1000],
    'regressor__penalty': ['l1','l2'],
    'regressor__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]
}

# exaustively explores values from param_grid
grid_search = GridSearchCV(
    logregr, param_grid, cv=5, verbose=3, n_jobs=2,
    scoring='f1_micro')

# fit the model to the training data
grid_search.fit(X_train, y_train)

# print mean cross-validated score of the best_estimator
print('best score {}'.format(grid_search.best_score_))

# try random forest 
from sklearn.ensemble import RandomForestClassifier

# estimator using random forest
rfc = Pipeline(steps=[('preprocessor', preprocessor),
                      ('regressor', RandomForestClassifier())])

# hyper-parameters to be explored and allowed values
param_grid = {
    'preprocessor__num__imputer__strategy': ['mean'],
    'regressor__n_estimators' : [350, 400, 450, 500],
    'regressor__max_features' : [3, 6, 9, 15],
    'regressor__max_depth': [5, 10, 15]
}

# exaustively explores values from param_grid
grid_search = GridSearchCV(
    rfc, param_grid, verbose=3, n_jobs=2,
    scoring='f1_micro')

# fit the model to the training data
grid_search.fit(X_train, y_train)

# print score of the best_estimator
print('best score {}'.format(grid_search.best_score_))

# parameters of best estimator after tuning 
grid_search.best_estimator_

# Prediction & generating the submission file
y_pred = grid_search.predict(X_test)
pd.DataFrame(
    {'Id': testing_ids, 'price_rating':y_pred}).to_csv('submission.csv', index=False)